{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDzA_PBkRHo-"
   },
   "source": [
    "# **Step 1: Data Collection (Reddit Scraping)**\n",
    "We'll start by scraping a balanced dataset (1000 spoiler, 1000 non-spoiler comments) using your provided Reddit credentials via asyncpraw.\n",
    "\n",
    "Goals:\n",
    "* Target subreddits: r/movies, r/television, r/marvelstudios, r/MovieDetails\n",
    "* Filter posts using keyword \"spoiler\"\n",
    "* Identify spoilers via >!spoiler!< markdown\n",
    "* Save structured CSV with columns: Movie, Comment, Comment Type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fcmEBBwRQwT"
   },
   "source": [
    "### **Step 1.1: Install Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxDZh-vrQ4KA",
    "outputId": "f88adf5c-2f49-4233-eab7-c0d6ef0dbdac"
   },
   "outputs": [],
   "source": [
    "!pip install asyncpraw nest_asyncio pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP5-RQhrRYC9"
   },
   "source": [
    "### **Step 1.2: Reddit Scraper Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mz99rXRwRTHC",
    "outputId": "06731a0b-376a-4bc7-b334-329cfe254208"
   },
   "outputs": [],
   "source": [
    "import asyncpraw\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#reddit credentials\n",
    "client_id = \"EpA1si88zgqhg45ZWOsOiA\"\n",
    "client_secret = \"6Im4euHKIkqNHhyyhKg7sAGz_XaQug\"\n",
    "user_agent = \"SpoilerShieldBot/0.1 by Temporary_Reason5148\"\n",
    "\n",
    "#asyncPRAW reddit instance\n",
    "reddit = asyncpraw.Reddit(client_id=client_id,\n",
    "                          client_secret=client_secret,\n",
    "                          user_agent=user_agent)\n",
    "\n",
    "#subreddits and keywords\n",
    "subreddits = [\"movies\", \"television\", \"marvelstudios\", \"MovieDetails\"]\n",
    "keyword = \"spoiler\"\n",
    "\n",
    "#storage\n",
    "spoilers, non_spoilers = [], []\n",
    "\n",
    "async def fetch_comments():\n",
    "    global spoilers, non_spoilers\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        print(f\"Scraping subreddit: r/{subreddit}\")\n",
    "        subreddit_obj = await reddit.subreddit(subreddit)\n",
    "\n",
    "        async for submission in subreddit_obj.search(keyword, limit=300):\n",
    "            title = submission.title\n",
    "            await submission.load()\n",
    "            await submission.comments.replace_more(limit=0)\n",
    "            for comment in submission.comments.list():\n",
    "                if not comment.body or len(comment.body) < 10:\n",
    "                    continue\n",
    "                body = comment.body.strip()\n",
    "                if \">!\" in body and \"!<\" in body:\n",
    "                    if len(spoilers) < 1000:\n",
    "                        spoilers.append({\"Movie\": title, \"Comment\": body, \"Comment Type\": \"Spoiler\"})\n",
    "                elif len(non_spoilers) < 1000:\n",
    "                    non_spoilers.append({\"Movie\": title, \"Comment\": body, \"Comment Type\": \"Non-Spoiler\"})\n",
    "\n",
    "                if len(spoilers) >= 1000 and len(non_spoilers) >= 1000:\n",
    "                    break\n",
    "\n",
    "            if len(spoilers) >= 1000 and len(non_spoilers) >= 1000:\n",
    "                break\n",
    "\n",
    "    print(f\"Scraping complete: {len(spoilers)} spoilers, {len(non_spoilers)} non-spoilers\")\n",
    "\n",
    "#run the async task\n",
    "await fetch_comments()\n",
    "\n",
    "#combine and save to CSV\n",
    "data = pd.DataFrame(spoilers + non_spoilers)\n",
    "data.to_csv(\"spoiler_shield_dataset.csv\", index=False)\n",
    "print(\"Saved to spoiler_shield_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LfnZwNGRdVu"
   },
   "source": [
    "# **Step 2: Data Preprocessing**\n",
    "The goal is to clean and normalize the comment text for model training:\n",
    "*   Remove spoiler markdown (>!spoiler!< → spoiler)\n",
    "*   Remove unwanted characters, links, special symbols\n",
    "*   Lowercase text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FNtVvE5Rjlw"
   },
   "source": [
    "### **Step 2.1: Install Preprocessing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEzWe2-kRZkn",
    "outputId": "2731fdf8-5710-4298-ebd9-3c2f01d13a8d"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Wp1_BY9Rmoz"
   },
   "source": [
    "### **Step 2.2: Clean and Normalize Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnAX0MyfRovB",
    "outputId": "9751b71a-0784-4d80-feca-21e6c0a354e3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#load dataset\n",
    "df = pd.read_csv(\"spoiler_shield_dataset.csv\")\n",
    "\n",
    "#preprocessing function\n",
    "def clean_comment(text):\n",
    "    #remove spoiler markdown >!text!< → text\n",
    "    text = re.sub(r'>!(.*?)!<', r'\\1', text)\n",
    "\n",
    "    #remove links, markdown, punctuation\n",
    "    text = re.sub(r'http\\S+|www\\S+|[\\*\\[\\]\\(\\)\\{\\}]|[\\n\\r]', '', text)\n",
    "\n",
    "    #remove special characters, keep only words\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    #lowercase and remove extra spaces\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    #remove stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "#apply cleaning\n",
    "df[\"Cleaned Comment\"] = df[\"Comment\"].apply(clean_comment)\n",
    "\n",
    "#check balance\n",
    "print(df[\"Comment Type\"].value_counts())\n",
    "\n",
    "#save preprocessed dataset\n",
    "df.to_csv(\"spoiler_shield_cleaned.csv\", index=False)\n",
    "print(\"Preprocessing complete. Saved as spoiler_shield_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJdQINBkRr60"
   },
   "source": [
    "# **Step 3: Contrastive Text Embedding**\n",
    "We'll now create positive and negative text pairs from the cleaned dataset to train a contrastive model.\n",
    "The goal is:\n",
    "\n",
    "*   Positive pairs: same label (e.g., two spoiler comments)\n",
    "*   Negative pairs: different labels (e.g., spoiler vs non-spoiler)\n",
    "*   We'll use SentenceTransformer to train a model with a CosineSimilarityLoss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zec_Mk4jRwiK"
   },
   "source": [
    "### **Step 3.1: Install Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jV6OcCWRv6v",
    "outputId": "e0469045-ce5d-4da0-cc02-9ef1ab591381"
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LySENvdvR3nR"
   },
   "source": [
    "### **Step 3.2: Generate Contrastive Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lsIWbgj4R3N_",
    "outputId": "8666a6dd-7413-4398-d366-f08b8ad380bb"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "import random\n",
    "\n",
    "#load the cleaned dataset\n",
    "df = pd.read_csv(\"spoiler_shield_cleaned.csv\")\n",
    "\n",
    "#convert rows into InputExample format\n",
    "def generate_contrastive_pairs(df, max_pairs=1000):\n",
    "    examples = []\n",
    "    spoiler_df = df[df['Comment Type'] == 'Spoiler']\n",
    "    non_spoiler_df = df[df['Comment Type'] == 'Non-Spoiler']\n",
    "\n",
    "    #shuffle to ensure variety\n",
    "    spoiler_df = spoiler_df.sample(frac=1).reset_index(drop=True)\n",
    "    non_spoiler_df = non_spoiler_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    num_pairs = min(len(spoiler_df), len(non_spoiler_df), max_pairs)\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        #positive pair (same class)\n",
    "        examples.append(InputExample(\n",
    "            texts=[spoiler_df.iloc[i]['Cleaned Comment'], spoiler_df.iloc[(i+1) % num_pairs]['Cleaned Comment']],\n",
    "            label=1.0\n",
    "        ))\n",
    "        examples.append(InputExample(\n",
    "            texts=[non_spoiler_df.iloc[i]['Cleaned Comment'], non_spoiler_df.iloc[(i+1) % num_pairs]['Cleaned Comment']],\n",
    "            label=1.0\n",
    "        ))\n",
    "\n",
    "        #negative pair (different class)\n",
    "        examples.append(InputExample(\n",
    "            texts=[spoiler_df.iloc[i]['Cleaned Comment'], non_spoiler_df.iloc[i]['Cleaned Comment']],\n",
    "            label=0.0\n",
    "        ))\n",
    "\n",
    "    print(f\"Total pairs created: {len(examples)}\")\n",
    "    return examples\n",
    "\n",
    "#generate and store pairs\n",
    "train_examples = generate_contrastive_pairs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Azi-v9y9xnfX",
    "outputId": "acd207f1-cb60-4d4e-da7a-30bf38d2a8d0"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#load the cleaned dataset\n",
    "df = pd.read_csv(\"spoiler_shield_cleaned.csv\")\n",
    "\n",
    "#convert 'Cleaned Comment' to string type and fill NaNs with empty string\n",
    "#this handles potential non-string entries including NaNs\n",
    "df['Cleaned Comment'] = df['Cleaned Comment'].astype(str).fillna('')\n",
    "\n",
    "#filter out rows where 'Cleaned Comment' is an empty string after conversion\n",
    "df = df[df['Cleaned Comment'] != ''].reset_index(drop=True)\n",
    "\n",
    "\n",
    "#convert rows into InputExample format\n",
    "def generate_contrastive_pairs(df, max_pairs=1000):\n",
    "    examples = []\n",
    "    #ensure Comment Type is also consistent for filtering\n",
    "    df['Comment Type'] = df['Comment Type'].astype(str).str.lower()\n",
    "    spoiler_df = df[df['Comment Type'] == 'spoiler']\n",
    "    non_spoiler_df = df[df['Comment Type'] == 'non-spoiler']\n",
    "\n",
    "\n",
    "    #shuffle to ensure variety\n",
    "    spoiler_df = spoiler_df.sample(frac=1).reset_index(drop=True)\n",
    "    non_spoiler_df = non_spoiler_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    num_pairs = min(len(spoiler_df), len(non_spoiler_df), max_pairs)\n",
    "\n",
    "    #check if there are enough samples to create pairs\n",
    "    if num_pairs < 2:\n",
    "        print(f\"Not enough data ({num_pairs} of each type) to create meaningful pairs.\")\n",
    "        return []\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        #positive pair (same class)\n",
    "        #use modulo to wrap around and ensure we always have a second example\n",
    "        examples.append(InputExample(\n",
    "            texts=[spoiler_df.iloc[i]['Cleaned Comment'], spoiler_df.iloc[(i+1) % num_pairs]['Cleaned Comment']],\n",
    "            label=1.0\n",
    "        ))\n",
    "        examples.append(InputExample(\n",
    "            texts=[non_spoiler_df.iloc[i]['Cleaned Comment'], non_spoiler_df.iloc[(i+1) % num_pairs]['Cleaned Comment']],\n",
    "            label=1.0\n",
    "        ))\n",
    "\n",
    "        #negative pair (different class)\n",
    "        examples.append(InputExample(\n",
    "            texts=[spoiler_df.iloc[i]['Cleaned Comment'], non_spoiler_df.iloc[i]['Cleaned Comment']],\n",
    "            label=0.0\n",
    "        ))\n",
    "\n",
    "    print(f\"Total pairs created: {len(examples)}\")\n",
    "    return examples\n",
    "\n",
    "#generate and store pairs\n",
    "train_examples = generate_contrastive_pairs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50TdkDaUR-td"
   },
   "source": [
    "# **Step 4: Model Training with Contrastive Loss**\n",
    "We’ll now train a Sentence-BERT model using CosineSimilarityLoss, a perfect fit for contrastive learning. The goal is to embed similar comments close together in vector space and dissimilar ones far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bv3XLCL9SAc6"
   },
   "source": [
    "### **Step 4.1: Load Pretrained SentenceTransformer Model**\n",
    "We’ll use distilbert-base-uncased as the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515,
     "referenced_widgets": [
      "3b0f9c14795745beb616fecbf6522eda",
      "2c804f54aae546acb2c874c896b4de3a",
      "62ac350736e54a72aa168b48135f1a52",
      "0b914ba6df9947709c7063d654b28a3b",
      "b5dff48c2ea04133baacaaa26643ed57",
      "bb0db12d6ad14b1db314876902a59f63",
      "dd7472f0d9674c038cdb557b4db7a7f0",
      "8c9787a024a14604a6ca9751e73e0774",
      "93bfb8db3e464ed79b6dec8b42c6e3e8",
      "37198c417d6b43fe8c07ad84c9e91fb8",
      "3d7dec98df974ffaa09c9ab48ccf57da",
      "68ba496671ca4331b39b3564a9344f5a",
      "d320aa684b004702b9c76f814011241c",
      "21470c0aeb914e0cb9a108eb83670593",
      "e685da7a55b34c03aecc878958320b9d",
      "89a667b7ef694d76aa278c5f03f9d29a",
      "6b1fda8800274da28aa51f5459cf85b8",
      "f204f52d50494f2e99314b487d603f81",
      "abd00ab40570426b97fa92e5965197f1",
      "db5edc95f81e4644b475df58d4784ff3",
      "442a86a484564fb59b7581ec62835f20",
      "af8af4ca90844043aadbd1b1c2430aff",
      "69c8d4f246354ca7abaf011f6e16ac77",
      "0efe7518b4304ed482bab3c280067ec6",
      "4753b9781ce04bd292e1f0b470ecba75",
      "c8db70179cdd490784574f4cacbf6fc4",
      "0b23b02d2fef4461bd75e934789269ff",
      "ffe81c943590403db8d1476c8682eb14",
      "102b027652a243f780fca296a64b3903",
      "51c37d3bfd644aafbeda038446703459",
      "42757d2171584aca9abe48549a41ef7e",
      "9a3e90f87f4649b4953085bcf0da2494",
      "c62775756aab4e0e84f984959a587221",
      "7c67c4c33c9d4f6fa31e2599c7cfc447",
      "3e830c8cea5a48879690d4a4095e9582",
      "d84ad175466140338b27fd8648e47465",
      "5cbb62b17d9e40efaf223148b3379780",
      "e67762a50a474f01a4a21ae0dbb88ada",
      "2fded2c204ab4af6860760a64e32612b",
      "e328d7b020d346c6b229446fc78de529",
      "813040758d0d408f84fee2c659d9e64e",
      "b70520dac4b0473d932a78412fff01ec",
      "2613cc7e4c714c1fbb24182af22db4c2",
      "1562d1574e144a7397c6f352e3464224",
      "5edb670ae4f44d2ab1a2725f0a7af042",
      "54563359abfa40fc8543a2edbf82c52f",
      "9038a983e8af4c7e89c2d017862d3e24",
      "7fa2913b38fd42d09b4183d40393ec2c",
      "7912a9a9db69444eb7cc59d50eaccfdd",
      "12419491aaf14c80b02bef32b3190b13",
      "5cd47b122f7e4bd6a086994b85a472d4",
      "19966802bc634999bb454b1e263c0219",
      "a300353b5e32497294abb5cd676dd2d2",
      "689b439e352b4c9098514ffe967050ea",
      "6f639b9e32044c67a7eec539d76c3c8a",
      "22d2fce027684869bcc24a1000af6b12",
      "98c7429510054415899fe8cb53a1f392",
      "cacf19aef29f4224a1da07a9b18b5e28",
      "24a4a1641f844f0f9b3eaee86d622cb8",
      "bd721630e1ec4893ad18c5155ac24c35",
      "e1304826b0be49b986bfdef7b73ce2f1",
      "3ac41158b91a4d01a5e746fb0f73e02a",
      "14646f5d72014aa1a5b4c855f92ee0bd",
      "a4ce8aade7d544d686aa0d973e386544",
      "f26d357a00be44a2bca231e8c9f5b445",
      "91ba976e0d4c4b699e9496e1032cba41"
     ]
    },
    "id": "YLpDrSgVR___",
    "outputId": "1492f6bc-c2b4-4264-a7e5-67d7710e1297"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "#disable WANDB for safety\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#limiting pairs\n",
    "train_examples = train_examples[:2000]\n",
    "\n",
    "#lightweight transformer base\n",
    "word_embedding_model = models.Transformer('distilbert-base-uncased', max_seq_length=128)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "#dataLoader with moderate batch size\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#train\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "#save model\n",
    "model.save(\"spoiler-shield-contrastive-model\")\n",
    "print(\"Model trained and saved as 'spoiler-shield-contrastive-model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "0e545e8126f74581b25b4b64c5874b85",
      "8d631169f094401aade1bc5aa79e123e",
      "a72eb211da1e41bcae58ae5c302dfc82",
      "1b7d7f16a5db4e59aba5cfb981930293",
      "c33ab43c5cb14a19a4dacd999a3b8c55",
      "f8e58dbf33d0403c8c4fb14448c69ca2",
      "0ad23c28e6714ac4939da42c022e3e24",
      "68559ce155074008976ce767f6eac7dd",
      "001141b8a85e4da5b7501b63e4a79f35",
      "aced7f8a2a3d47ff8d13d2d8f01f7537",
      "1f146dd521b24f35bcf5179e52c78143",
      "4c58000a9ba5470f99cc47f0f69edf30",
      "8cc58367d38d48f5a76629f4886663ee",
      "2a470a909b7f4d6d83ddd99dc46a9194",
      "4a28b1f275824a81ba088ae72e569389",
      "04ace310f2a94768ad5c621c118fac13",
      "99e59e0bf5e14da9a0ea44c17fc7e2fa",
      "8d757e30b86140d08a4e7760f4eeadb8",
      "e1d4586378b74b0baa6a895216ede26c",
      "8880d175661447108c8e9244f3cac7f4",
      "20ecb66aedaa4a17a39024de4e81d0f0",
      "b32aa09cc8e64344a99ab3f8c4e454da"
     ]
    },
    "id": "k6HbgbElmde9",
    "outputId": "598609bb-5c11-4cb9-f3dc-dce5b440751c"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "#load dataset\n",
    "df = pd.read_csv(\"spoiler_shield_cleaned.csv\")\n",
    "\n",
    "#normalize column names\n",
    "df.columns = [col.lower().strip() for col in df.columns]\n",
    "\n",
    "#lowercase comment type\n",
    "df['comment type'] = df['comment type'].str.lower().str.strip()\n",
    "\n",
    "#filter spoiler and non-spoiler cleaned comments\n",
    "spoiler_texts = df[(df['comment type'] == 'spoiler') & (df['cleaned comment'].str.len() > 50)]['cleaned comment'].tolist()\n",
    "non_spoiler_texts = df[(df['comment type'] == 'non-spoiler') & (df['cleaned comment'].str.len() > 50)]['cleaned comment'].tolist()\n",
    "\n",
    "#load model\n",
    "model = SentenceTransformer(\"spoiler-shield-contrastive-model\")\n",
    "\n",
    "#encode texts\n",
    "spoiler_embs = model.encode(spoiler_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "non_spoiler_embs = model.encode(non_spoiler_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "#compute mean vectors (anchors)\n",
    "spoiler_anchor = spoiler_embs.mean(dim=0, keepdim=True)\n",
    "non_spoiler_anchor = non_spoiler_embs.mean(dim=0, keepdim=True)\n",
    "\n",
    "#save to disk\n",
    "torch.save(spoiler_anchor, \"/content/spoiler_anchor.pt\")\n",
    "torch.save(non_spoiler_anchor, \"/content/non_spoiler_anchor.pt\")\n",
    "\n",
    "print(\"Spoiler and Non-Spoiler anchors saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bv6x88CmSFbn"
   },
   "source": [
    "# **Step 5: Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRcZMeFpWg-L",
    "outputId": "eaee9c65-c3d8-4cd7-91a6-a98000e08672"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#load model\n",
    "model = SentenceTransformer(\"spoiler-shield-contrastive-model\")\n",
    "\n",
    "#load preprocessed data\n",
    "df = pd.read_csv(\"spoiler_shield_cleaned.csv\")\n",
    "\n",
    "#create balanced test set\n",
    "spoiler_samples = df[df['Comment Type'] == 'Spoiler'].sample(n=300, random_state=42)\n",
    "non_spoiler_samples = df[df['Comment Type'] == 'Non-Spoiler'].sample(n=300, random_state=42)\n",
    "\n",
    "#construct test pairs\n",
    "test_texts1, test_texts2, labels = [], [], []\n",
    "\n",
    "for i in range(300):\n",
    "    #positive: spoiler-spoiler\n",
    "    test_texts1.append(spoiler_samples.iloc[i]['Cleaned Comment'])\n",
    "    test_texts2.append(spoiler_samples.iloc[(i+1) % 300]['Cleaned Comment'])\n",
    "    labels.append(1)\n",
    "\n",
    "    #positive: non-spoiler–non-spoiler\n",
    "    test_texts1.append(non_spoiler_samples.iloc[i]['Cleaned Comment'])\n",
    "    test_texts2.append(non_spoiler_samples.iloc[(i+1) % 300]['Cleaned Comment'])\n",
    "    labels.append(1)\n",
    "\n",
    "    #negative: spoiler–non-spoiler\n",
    "    test_texts1.append(spoiler_samples.iloc[i]['Cleaned Comment'])\n",
    "    test_texts2.append(non_spoiler_samples.iloc[i]['Cleaned Comment'])\n",
    "    labels.append(0)\n",
    "\n",
    "#encode pairs\n",
    "embeddings1 = model.encode(test_texts1, convert_to_tensor=True, batch_size=32)\n",
    "embeddings2 = model.encode(test_texts2, convert_to_tensor=True, batch_size=32)\n",
    "\n",
    "#compute cosine similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "cosine_scores = cosine_scores.diagonal().cpu().numpy()\n",
    "\n",
    "#convert similarity to binary predictions (threshold = 0.5)\n",
    "preds = [1 if score >= 0.5 else 0 for score in cosine_scores]\n",
    "\n",
    "#print metrics\n",
    "print(\"Evaluation Metrics\")\n",
    "print(\"Accuracy :\", accuracy_score(labels, preds))\n",
    "print(\"Precision:\", precision_score(labels, preds))\n",
    "print(\"Recall   :\", recall_score(labels, preds))\n",
    "print(\"F1-score :\", f1_score(labels, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd4OO-Bf2l9r"
   },
   "source": [
    "# **Step 6: Real-Time Spoiler Detector (Python Function)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUHCr8aH1xdA",
    "outputId": "2b710516-0201-4f33-9cbe-f07f55e1748f"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#load trained model\n",
    "model = SentenceTransformer('spoiler-shield-contrastive-model')\n",
    "\n",
    "#load cleaned dataset for anchor embeddings\n",
    "df = pd.read_csv(\"spoiler_shield_cleaned.csv\")\n",
    "\n",
    "#get balanced anchors\n",
    "spoiler_anchors = df[df[\"Comment Type\"] == \"Spoiler\"][\"Cleaned Comment\"].sample(n=100, random_state=42).tolist()\n",
    "non_spoiler_anchors = df[df[\"Comment Type\"] == \"Non-Spoiler\"][\"Cleaned Comment\"].sample(n=100, random_state=42).tolist()\n",
    "\n",
    "spoiler_embeddings = model.encode(spoiler_anchors, convert_to_tensor=True)\n",
    "non_spoiler_embeddings = model.encode(non_spoiler_anchors, convert_to_tensor=True)\n",
    "\n",
    "#preprocessing function\n",
    "def clean_comment(text):\n",
    "    text = re.sub(r'>!(.*?)!<', r'\\1', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|[\\*\\[\\]\\(\\)\\{\\}]|[\\n\\r]', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "#spoiler prediction function\n",
    "def predict_spoiler(user_input):\n",
    "    cleaned = clean_comment(user_input)\n",
    "    input_embedding = model.encode(cleaned, convert_to_tensor=True)\n",
    "\n",
    "    spoiler_score = util.cos_sim(input_embedding, spoiler_embeddings).mean()\n",
    "    non_spoiler_score = util.cos_sim(input_embedding, non_spoiler_embeddings).mean()\n",
    "\n",
    "    print(f\"Spoiler score: {spoiler_score:.4f}\")\n",
    "    print(f\"Non-spoiler score: {non_spoiler_score:.4f}\")\n",
    "\n",
    "    if spoiler_score > non_spoiler_score:\n",
    "        return \"Predicted: **Spoiler**\"\n",
    "    else:\n",
    "        return \"Predicted: **Non-Spoiler**\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmYh11iR2kT8",
    "outputId": "3c9bc40b-535b-48fc-abc6-ea27eddfa835"
   },
   "outputs": [],
   "source": [
    "comment = \"I like the movie director\"\n",
    "print(predict_spoiler(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npCoku2t984m"
   },
   "source": [
    "# **Step 7: Deployment**\n",
    "\n",
    "### **Step 7.1: Prepare Your Colab Environment**\n",
    "\n",
    "Install Streamlit & pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_k0Q-djK-BYX",
    "outputId": "4370a2b7-d7a8-4109-8fa7-dcdac0d1b833"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit pyngrok --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IW86GdX-FET"
   },
   "source": [
    "### **Step 7.2: Save Your Streamlit App Script**\n",
    "\n",
    "Create a new file called app.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBswPqd3-C0h",
    "outputId": "bfdbb584-5397-4adf-9c3f-64fc40f59f5c"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#helper function for safe sampling\n",
    "def safe_sample(df, n):\n",
    "    n = min(n, len(df))\n",
    "    if n > 0:\n",
    "        return df.sample(n=n)\n",
    "    else:\n",
    "        st.warning(\"Cannot sample from empty dataframe\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "#load dataset\n",
    "df = pd.read_csv(\"spoiler_shield_cleaned.csv\")\n",
    "\n",
    "#debug prints for dataset info\n",
    "st.write(\"Dataset loaded with shape:\", df.shape)\n",
    "\n",
    "#filter spoiler/non-spoiler DataFrames\n",
    "non_spoiler_df = df[df[\"Comment Type\"].str.lower() == \"non-spoiler\"]\n",
    "spoiler_df = df[df[\"Comment Type\"].str.lower() == \"spoiler\"]\n",
    "\n",
    "st.write(\"Spoiler comments count:\", len(spoiler_df))\n",
    "st.write(\"Non-spoiler comments count:\", len(non_spoiler_df))\n",
    "\n",
    "#check empty\n",
    "if spoiler_df.empty:\n",
    "    st.error(\"No spoiler comments found in the dataset.\")\n",
    "    st.stop()\n",
    "\n",
    "if non_spoiler_df.empty:\n",
    "    st.error(\"No non-spoiler comments found in the dataset.\")\n",
    "    st.stop()\n",
    "\n",
    "#Optionally safely sample some anchors if needed (otherwise just get all)\n",
    "#spoiler_sample_df = safe_sample(spoiler_df, 1000)\n",
    "#non_spoiler_sample_df = safe_sample(non_spoiler_df, 1000)\n",
    "\n",
    "#but if you want all data as anchors:\n",
    "spoiler_anchors = spoiler_df[\"Cleaned Comment\"].dropna().tolist()\n",
    "non_spoiler_anchors = non_spoiler_df[\"Cleaned Comment\"].dropna().tolist()\n",
    "\n",
    "if not spoiler_anchors:\n",
    "    st.error(\"Cleaned spoiler comments list is empty.\")\n",
    "    st.stop()\n",
    "\n",
    "if not non_spoiler_anchors:\n",
    "    st.error(\"Cleaned non-spoiler comments list is empty.\")\n",
    "    st.stop()\n",
    "\n",
    "#load model once\n",
    "model = SentenceTransformer(\"spoiler-shield-contrastive-model\")\n",
    "\n",
    "#encode embeddings once\n",
    "spoiler_embeddings = model.encode(spoiler_anchors, convert_to_tensor=True)\n",
    "non_spoiler_embeddings = model.encode(non_spoiler_anchors, convert_to_tensor=True)\n",
    "\n",
    "#text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'>!(.*?)!<', r'\\1', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "#spoiler prediction function\n",
    "def predict_spoiler(comment):\n",
    "    cleaned = clean_text(comment)\n",
    "    embedding = model.encode(cleaned, convert_to_tensor=True)\n",
    "    spoiler_sim = util.cos_sim(embedding, spoiler_embeddings).mean().item()\n",
    "    non_spoiler_sim = util.cos_sim(embedding, non_spoiler_embeddings).mean().item()\n",
    "    label = \"Spoiler\" if spoiler_sim > non_spoiler_sim else \"Non-Spoiler\"\n",
    "    return label, spoiler_sim, non_spoiler_sim\n",
    "\n",
    "#streamlit UI\n",
    "st.title(\"Spoiler Shield with NLP\")\n",
    "st.markdown(\"Enter a comment below to detect if it contains spoilers.\")\n",
    "\n",
    "user_input = st.text_area(\"Enter a comment:\")\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    if user_input.strip() == \"\":\n",
    "        st.warning(\"Please enter a comment first.\")\n",
    "    else:\n",
    "        label, sim_spoiler, sim_nonspoiler = predict_spoiler(user_input)\n",
    "        st.markdown(f\"### Prediction: `{label}`\")\n",
    "        st.write(f\"**Spoiler Similarity:** {sim_spoiler:.4f}\")\n",
    "        st.write(f\"**Non-Spoiler Similarity:** {sim_nonspoiler:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "YGb_d1nYjcUL",
    "outputId": "966e31a6-8c9f-4261-a26a-3820b44c2e49"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3-AgKrv-pfk"
   },
   "source": [
    "## **Step 7.3: Launch the Streamlit App with pyngrok**\n",
    "\n",
    "Now let's create a public demo link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7subplsjlp7",
    "outputId": "0f6b8730-f837-4685-e7b0-a8d8d15b164b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyngrok import ngrok\n",
    "\n",
    "#kill any existing tunnels\n",
    "ngrok.kill()\n",
    "\n",
    "ngrok.set_auth_token(\"2xb2VrWhxsTBH3RWm5MgJp2zTX3_NDMTjcMX8zci6qp6d8yU\")\n",
    "#start Streamlit in the background\n",
    "os.system(\"streamlit run app.py &\")\n",
    "\n",
    "#give Streamlit time to start\n",
    "time.sleep(5)\n",
    "\n",
    "#connect ngrok to the default Streamlit port (8501)\n",
    "public_url = ngrok.connect(\"http://localhost:8501\", bind_tls=True)\n",
    "print(f\"Your Streamlit app is live at: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcHjaB666CAI"
   },
   "outputs": [],
   "source": [
    "#!streamlit run app.py &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1dFJ_7kaoRC",
    "outputId": "f8bd8143-f1b6-493e-acf2-af19813b5c24"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5Rg64Cgaydr"
   },
   "outputs": [],
   "source": [
    "#!zip -r spoiler-shield-nlp.zip /content/spoiler-shield-nlp/spoiler-shield-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rdvP89_6LQ9"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download('spoiler-shield-nlp.zip')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
